{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "## References\n",
    "\n",
    "+ Lectures 13-16 (inclusive).\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you can either:\n",
    "    \n",
    "    - Type the answer using the built-in latex capabilities. In this case, simply export the notebook as a pdf and upload it on gradescope; or\n",
    "    - You can print the notebook (after you are done with all the code), write your answers by hand, scan, turn your response to a single pdf, and upload on gradescope.\n",
    "\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "**Note**: Please match all the pages corresponding to each of the questions when you submit on gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student details\n",
    "\n",
    "+ **First Name:**\n",
    "+ **Last Name:**\n",
    "+ **Email:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "import scipy.stats as st\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Estimating the mechanical properties of a plastic material from molecular dynamics simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure that [this](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/stress_strain.txt) dataset is visible from this Jupyter notebook.\n",
    "You may achieve this by either:\n",
    "\n",
    "+ Downloading the data file, putting it in your Google drive, mounting the drive, and changing to the directory of the file (see Problem 0 in [Homework](https://colab.research.google.com/github/PredictiveScienceLab/data-analytics-se/blob/master/homework/homework_03.ipynb); or\n",
    "+ Downloading the file to the working directory of this notebook with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/stress_strain.txt'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's up to you what you choose to do.\n",
    "If the file is in the right place, the following code should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '404:'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m  \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstress_strain.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gst/lib/python3.8/site-packages/numpy/lib/npyio.py:1148\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# read data in chunks and fill it into an array via resize\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# over-allocating and shrinking the array later may be faster but is\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m# probably not relevant compared to the cost of actually reading and\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# converting the data\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m read_data(_loadtxt_chunksize):\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m         X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x, dtype)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gst/lib/python3.8/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36mloadtxt.<locals>.read_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong number of columns at line \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                      \u001b[38;5;241m%\u001b[39m line_num)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# Convert each value according to its column and store\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m items \u001b[38;5;241m=\u001b[39m [conv(val) \u001b[38;5;28;01mfor\u001b[39;00m (conv, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(converters, vals)]\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# Then pack it according to the dtype's nesting\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m items \u001b[38;5;241m=\u001b[39m pack_items(items, packing)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gst/lib/python3.8/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong number of columns at line \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                      \u001b[38;5;241m%\u001b[39m line_num)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# Convert each value according to its column and store\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m items \u001b[38;5;241m=\u001b[39m [\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (conv, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(converters, vals)]\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# Then pack it according to the dtype's nesting\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m items \u001b[38;5;241m=\u001b[39m pack_items(items, packing)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gst/lib/python3.8/site-packages/numpy/lib/npyio.py:736\u001b[0m, in \u001b[0;36m_getconv.<locals>.floatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0x\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m.\u001b[39mfromhex(x)\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '404:'"
     ]
    }
   ],
   "source": [
    "data =  np.loadtxt('stress_strain.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was generated using a molecular dynamics simulation of a plastic material (thanks to [Professor Alejandro Strachan](https://engineering.purdue.edu/MSE/people/ptProfile?id=33239) for sharing the data!).\n",
    "Specifically, Strachan's group did the following:\n",
    "- They took a rectangular chunk of the material and marked the position of each one of its atoms;\n",
    "- They started applying a tensile force along one dimension.\n",
    "The atoms are coupled together through electromagnetic forces and they must all satisfy Newton's law of motion.\n",
    "- For each value of the applied tensile force they marked the stress (force be unit area) in the middle of the materail and the corresponding strain of the material (percent enlogation in the pulling direction).\n",
    "- Eventually the material entered the plastic regime and then it broke.\n",
    "Here is a visualization of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# Strain \u001b[39;00m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Stress in MPa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "x = data[:, 0] # Strain \n",
    "y = data[:, 1] # Stress in MPa\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(x, y, 'ro', markersize=2, label = 'Stress-strain data')\n",
    "plt.xlabel('$\\epsilon$ (strain in %)', fontsize=14)\n",
    "plt.ylabel('$\\sigma$ (stress in MPa)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for each particular value of the strain, you don't necessarily get a unique stress.\n",
    "This is because in molecular dynamics the atoms are jiggling around due to thermal effects.\n",
    "So there is always this \"jiggling\" noise when you are trying to measure the stress and the strain.\n",
    "We would like to process this noise in order to extract what is known as the [stress-strain curve](https://en.wikipedia.org/wiki/Stress–strain_curve) of the material.\n",
    "The stress-strain curve is a macroscopic property of the the material which is affeted by the fine structure, e.g., the chemical bonds, the crystaline structure, any defects, etc.\n",
    "It is a required input to mechanics of materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Fitting the stress-strain curve in the elastic regime\n",
    "The very first part of the stress-strain curve should be linear.\n",
    "It is called the *elastic regime*.\n",
    "In that region, say $\\epsilon < \\epsilon_l=0.04$, the relationship between stress and strain is:\n",
    "$$\n",
    "\\sigma(\\epsilon) = E\\epsilon.\n",
    "$$\n",
    "The constant $E$ is known as the *Young modulus* of the material.\n",
    "Assume that you measure $\\epsilon$ without any noise, but your measured $\\sigma$ is noisy.\n",
    "\n",
    "### Subpart A.I\n",
    "First, extract the relevant data for this problem, split it into training and validation datasets, and visualize the training and validation datasets using different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The point at which the stress-strain curve stops being linear\n",
    "epsilon_l = 0.04\n",
    "# Relevant data (this is nice way to get the linear part of the stresses and straints)\n",
    "x_rel = x[x < 0.04]\n",
    "y_rel = y[x < 0.04]\n",
    "# Visualize to make sure you have the right data\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(x_rel, y_rel, 'ro', markersize=2, label = 'Stress-strain data')\n",
    "plt.xlabel('$\\epsilon$ (strain in %)', fontsize=14)\n",
    "plt.ylabel('$\\sigma$ (stress in MPa)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split your data into training and validation. Hint: You may use [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html](sklearn.model_selection.train_test_split) if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation datasets\n",
    "# Hint: Consult the hands-on activities of the lectures\n",
    "x_train = # Your code\n",
    "y_train = # Your code\n",
    "x_valid = # Your code\n",
    "y_valid = # Your code\n",
    "# Your code to plot the two datasets here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to visualize your split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_train, y_train, 'ro', markersize=2, label = 'Training data')\n",
    "plt.plot(x_valid, y_valid, 'bx', markersize=2, label = 'Validation data')\n",
    "plt.xlabel('$\\epsilon$ (strain in %)', fontsize=14)\n",
    "plt.ylabel('$\\sigma$ (stress in MPa)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.II\n",
    "Perform Bayesian linear regression with the evidence approximation to estimate the noise variance and the hyperparameters of the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.III\n",
    "Calculate the mean square error of the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.IV\n",
    "Make the observations vs predictions plot for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.V\n",
    "Compute and plot the standarized errors for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.VI\n",
    "Make the quantile-quantile plot of the standarized errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.VII\n",
    "Visualize your epistemic and the aleatory uncertainty about the stress-strain curve in the elastic regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A. VIII\n",
    "Visualize the posterior of the Young modulus E conditioned on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.IX\n",
    "Take five samples of stress-strain curve in the elastic regime and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.X\n",
    "\n",
    "Find the 95% centered credible interval for the Young modulus $E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.XI\n",
    "If you had to pick a single value for the Young modulus $E$, what would it be and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Estimate the ultimate strength\n",
    "\n",
    "The pick of the stress-strain curve is known as the ultimate strength.\n",
    "We will like to estimate it.\n",
    "\n",
    "### Subpart B.I - Extract training and validation data\n",
    "\n",
    "Extract training and validation data from the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - Repeat as many text and code blocks as you like\n",
    "x_train = # Your code\n",
    "y_train = # Your code\n",
    "x_valid = # Your code\n",
    "y_valid = # Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to visualize your split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_train, y_train, 'ro', markersize=2, label = 'Training data')\n",
    "plt.plot(x_valid, y_valid, 'bx', markersize=2, label = 'Validation data')\n",
    "plt.xlabel('$\\epsilon$ (strain in %)', fontsize=14)\n",
    "plt.ylabel('$\\sigma$ (stress in MPa)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.II - Model the entire stress-strain relationship.\n",
    "To do this, we will set up a generalized linear model that can capture the entire stress-strain relationship.\n",
    "Remember, you can use any model you want as soon as:\n",
    "+ it is linear in the parameters to be estimated,\n",
    "+ it clearly has a well-defined elastic regime (see Part A).\n",
    "\n",
    "I am going to help you set up the right model.\n",
    "We are goint to use the [Heavide step function](https://en.wikipedia.org/wiki/Heaviside_step_function) to turn on or off models for various ranges of $\\epsilon$. The idea is quite simple: We will use a linear model for the elastic regime and we are going to turn to a non-linear model for the non-linear regime.\n",
    "Here is a model that has the right form in the elastic regime and an arbitrary form in the non-linear regime:\n",
    "$$\n",
    "f(\\epsilon;E,\\mathbf{w}_g) = E\\epsilon \\left[(1 - H(\\epsilon - \\epsilon_l)\\right] + g(\\epsilon;\\mathbf{w}_g)H(\\epsilon - \\epsilon_l),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "H(x) = \\begin{cases}\n",
    "0,\\;\\text{if}\\;x < 0\\\\\n",
    "1,\\;\\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "and $g$ is any function linear in the parameters $\\mathbf{w}_g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any model you like for the non-linear regime, but let's use a polynomial of degree $d$:\n",
    "$$\n",
    "g(\\epsilon) = \\sum_{i=0}^{d} w_i \\epsilon^i.\n",
    "$$\n",
    "\n",
    "The full model can be expressed as:\n",
    "$$\n",
    "\\begin{split}\n",
    "f(\\epsilon) &= \n",
    "\\begin{cases}\n",
    "h(\\epsilon) = E \\epsilon,\\ \\epsilon < \\epsilon_l, \\\\\n",
    "g(\\epsilon) = \\sum_{i=0}^{d} w_i \\epsilon^i, \\epsilon \\geq \\epsilon_l\n",
    "\\end{cases}\\\\\n",
    "&= E\\epsilon \\left(1 - H(\\epsilon - \\epsilon_l)\\right) + \\sum_{i=0}^{d} w_i \\epsilon^iH(\\epsilon - \\epsilon_l).\n",
    "\\end{split}\n",
    "$$\n",
    "We could proceed with this model, but there is a small problem: It is discontinuous at $\\epsilon = \\epsilon_l$.\n",
    "This is unphysical. We can do better than that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model nice, we force the $h$ and $g$ to match up to the first derivative, i.e., we demand that:\n",
    "$$\n",
    "\\begin{split}\n",
    "h(\\epsilon_l) &= g(\\epsilon_l)\\\\\n",
    "h'(\\epsilon_l) &= g'(\\epsilon_l).\n",
    "\\end{split}\n",
    "$$\n",
    "The reason we include the first derivative is so that we don't have a kink in the stress-strain. That would also be unphysical.\n",
    "The two equations above become:\n",
    "$$\n",
    "\\begin{split}\n",
    "E\\epsilon_l &= \\sum_{i=0}^dw_i\\epsilon_l^i\\\\\n",
    "E &= \\sum_{i=1}^diw_i\\epsilon_l^{i-1}.\n",
    "\\end{split}\n",
    "$$\n",
    "We can use these two equations to eliminate two weights.\n",
    "Let's eliminate $w_0$ and $w_1$.\n",
    "All you have to do is express them in terms of $E$ and $w_2,\\dots,w_d$.\n",
    "So, there remain $d$ parameters to estimate.\n",
    "Let's get back to the stress-strain model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our stress-strain model was:\n",
    "$$\n",
    "f(\\epsilon) = E\\epsilon \\left(1 - H(\\epsilon - \\epsilon_l)\\right) + \\sum_{i=0}^{d} w_i \\epsilon^iH(\\epsilon - \\epsilon_l).\n",
    "$$\n",
    "We can now use the expressions for $w_0$ and $w_1$ to rewrite this using only all the other parameters.\n",
    "I am going to spare you the details...\n",
    "The end result is:\n",
    "$$\n",
    "f(\\epsilon) = E\\epsilon + \\sum_{i=2}^dw_i\\left[(i-1)\\epsilon_{l}^{i} - i \\epsilon \\epsilon_{l}^{i-1} + \\epsilon^i\\right]H(\\epsilon - \\epsilon_l).\n",
    "$$\n",
    "Okay.\n",
    "This is still a generalized linear model. This is nice.\n",
    "Write code for the design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete this code to make your model:\n",
    "def compute_design_matrix(Epsilon, epsilon_l, d):\n",
    "    \"\"\"\n",
    "    Computes the design matrix for the stress-strain curve problem.\n",
    "    \n",
    "    Arguments:\n",
    "        Epsilon     -     A 1D array of dimension N.\n",
    "        epsilon_l   -     The strain signifying the end of the elastic regime.\n",
    "        d           -     The polynomial degree.\n",
    "    \n",
    "    Returns:\n",
    "        A design matrix N x d\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    assert isinstance(Epsilon, np.ndarray)\n",
    "    assert Epsilon.ndim == 1, 'Pass the array as epsilon.flatten(), if it is two dimensional'\n",
    "    n = Epsilon.shape[0]\n",
    "    # The design matrix:\n",
    "    Phi = np.ndarray((n, d))\n",
    "    # The step function evaluated at all the elements of Epsilon.\n",
    "    # You can use it if you want.\n",
    "    Step = np.ones(n)\n",
    "    Step[Epsilon < epsilon_l] = 0\n",
    "    # Build the design matrix\n",
    "    Phi[:, 0] = # Your code here\n",
    "    for i in range(2, d+1):\n",
    "        Phi[:, i-1] = # Your code here\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the basis functions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "eps = np.linspace(0, x.max(), 100)\n",
    "Phis = compute_design_matrix(eps, epsilon_l, d)\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(eps, Phis)\n",
    "ax.set_xlabel('$\\epsilon$ (strain in %))')\n",
    "ax.set_ylabel('$\\phi_i(\\epsilon)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.III \n",
    "\n",
    "Fit the model using automatic relevance determination and demonstrate that it works well by doing all the things we did above (MSE, observations vs predictions plot, standarized errors, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - Use as many blocks as you need!\n",
    "model = # Just call the resulting model \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.IV\n",
    "Visualize epistemic and aleatory uncertainty in the stess-strain relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.V - Extract the ultimate strength\n",
    "\n",
    "Now, you are going to quantify your epistemic uncertainty about the ultimate strength.\n",
    "The ultimate strength is the maximum of the stress-strain relationship.\n",
    "Since you have epistemic uncertainty about the stress-strain relationship, you also have epistemic uncertainty about the ultimate strength.\n",
    "\n",
    "Do the following:\n",
    "- Visualize posterior of the ultimate strength.\n",
    "- Find a 95% credible interval for the ultimate strength.\n",
    "- Pick a value for the ultimate strength.\n",
    "\n",
    "**Hint:**\n",
    "To characterize your epistemic uncertainty about the ultimate strength, you would have to do the following:\n",
    "- Define a dense set of strain points between 0 and 0.25.\n",
    "- Repeatedly:\n",
    "    + sample from the posterior of the weights of your model\n",
    "    + for each sample evaluate the stresses at the dense set of strain points defined earlier\n",
    "    + for each sampled stress vector, find the maximum. This is a sample of the ultimate strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Optimizing the performance of a compressor\n",
    "\n",
    "In this problem we are going to need [this](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/compressor_data.xlsx) dataset. The dataset was kindly provided to us by [Professor Davide Ziviani](https://scholar.google.com/citations?user=gPdAtg0AAAAJ&hl=en).\n",
    "As before, you can either put it on your Google drive or just download it with the code segment below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/compressor_data.xlsx'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is an Excell file, so we are going to need pandas to read it.\n",
    "Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('compressor_data.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are part of a an experimental study of a variable speed reciprocating compressor.\n",
    "The experimentalists varied two temperatures $T_e$ and $T_c$ (both in C) and they measured various other quantities.\n",
    "Our goal is to learn the map between $T_e$ and $T_c$ and measured Capacity and Power (both in W).\n",
    "First, let's see how you can extract only the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how to extract the T_e and T_c columns and put them in a single numpy array\n",
    "x = data[['T_e','T_c']].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how to extract the Capacity\n",
    "y = data['Capacity'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the following multivariate polynomial model to **both the Capacity and the Power**:\n",
    "$$\n",
    "y = w_1 + w_2T_e + w_3 T_c + w_4 T_eT_c + w_5 T_e^2 + w_6T_c^2 + w_7 T_e^2T_c + w_8T_eT_c^2 + w_9 T_e^3 + w_{10}T_c^3 + \\epsilon,\n",
    "$$\n",
    "where $\\epsilon$ is a Gaussian noise term with unknown variance.\n",
    "**Hints:**\n",
    "+ You may use [sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to construct the design matrix of your polynomial features. Do not program the design matrix by hand.\n",
    "+ You should split your data into training and validation and use various validation metrics to make sure that your models make sense.\n",
    "+ Use [ARD Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression) to fit any hyperparameters and the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.I - Fit the capacity\n",
    "\n",
    "Please don't just fit blindly. Split in training and test and use all the usual diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - Repeat as many text and code blocks as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.II\n",
    "\n",
    "What is the noise variance you estimated for the Capacity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart A.III\n",
    "Which features of the temperatures (basis functions of your model) are the most important for predicting the Capacity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.I - Fit the Power\n",
    "\n",
    "Please don't just fit blindly. Split in training and test and use all the usual diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - Repeat as many text and code blocks as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.II\n",
    "\n",
    "What is the noise variance you estimated for the Power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.III\n",
    "Which features of the temperatures (basis functions of your model) are the most important for predicting the Power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Explaining the challenger disaster\n",
    "On January 28, 1986, the [Space Shuttle Challenger](https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster) disintegrated after 73 seconds from launch.\n",
    "The failure can be traced on the rubber O-rings which were used to seal the joints of the solid rocket boosters (required to force the hot, high-pressure gases generated by the burning solid propelant through the nozzles thus producing thrust).\n",
    "\n",
    "It turns out that the performance of the O-ring material was particularly sensitive on the external temperature during launch.\n",
    "This [dataset](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/challenger_data.csv) contains records of different experiments with O-rings recorded at various times between 1981 and 1986.\n",
    "Download the data the usual way (either put them on Google drive or run the code cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/challenger_data.csv'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is a csv file, you should load it with pandas because it contains some special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('challenger_data.csv')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the date of the record. The second column is the external temperature of that day in degrees F.\n",
    "The third column labeled ``Damage Incident`` is has a binary coding (0=no damage, 1=damage).\n",
    "The very last row is the day of the Challenger accident.\n",
    "\n",
    "We are going to use the first 23 rows to solve a binary classification problem that will give us the probability of an accident conditioned on the observed external temperature in degrees F. Before we proceed to the analysis of the data, let's clean the data up.\n",
    "\n",
    "First, we drop all the bad records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_0 = raw_data.dropna()\n",
    "clean_data_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also don't need the last record. Just remember that the temperature the day of the Challenger accident was 31 degrees F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data_0[:-1]\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the features and the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clean_data['Temperature'].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clean_data['Damage Incident'].values.astype(np.float)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Perform logistic regression\n",
    "\n",
    "Perform logistic regression between the temperature ($x$) and the damage label ($y$).\n",
    "Do not bother doing a validation because there are not a lot of data.\n",
    "Just use a very simple model so that you don't overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - Repeat as many text and code blocks as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Plot the probability of damage as a function of temperature\n",
    "Plot the probability of damage as a function of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Decide whether or not to launch\n",
    "\n",
    "The temperature the day of the Challenger accident was 31 degrees F.\n",
    "Start by calculating the probability of damage at 31 degrees F.\n",
    "Then, use formal decision-making (i.e., define a cost matrix and make decisions by minimizing the expected loss) to decide whether or not to launch on that day.\n",
    "Also, plot your optimal decision as a function of the external temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - Repeat as many text and code blocks as you like"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
