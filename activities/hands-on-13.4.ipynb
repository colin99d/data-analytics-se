{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 13.4: Measures of Predictive Accuracy\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To introduce measures that quantify how good a regression model is.\n",
    "\n",
    "## Validation dataset\n",
    "\n",
    "You cannot test how good your model is using the training dataset.\n",
    "Whatever the metric you use, the performance of your model on the training dataset will always be quite good.\n",
    "This is because the model is tuned to do well on the training data.\n",
    "The real question is how well your model does on a dataset it has never see.\n",
    "This brings us to the concept of a *validation dataset*.\n",
    "How can you make a validation dataset?\n",
    "Well, take whatever data you have a split it into training and validation dataset.\n",
    "For example, you can randomly select 70% of your data and put it in your training set and 30% of the data and put it in your validation set.\n",
    "\n",
    "## Example - Motorcyle data\n",
    "Let's do this for the motorcyle dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '404:'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m download(url)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmotor.dat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Figure out how many observations you have\u001b[39;00m\n\u001b[1;32m      8\u001b[0m num_obs \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gst/lib/python3.8/site-packages/numpy/lib/npyio.py:1148\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# read data in chunks and fill it into an array via resize\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# over-allocating and shrinking the array later may be faster but is\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m# probably not relevant compared to the cost of actually reading and\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# converting the data\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m read_data(_loadtxt_chunksize):\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m         X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x, dtype)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gst/lib/python3.8/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36mloadtxt.<locals>.read_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong number of columns at line \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                      \u001b[38;5;241m%\u001b[39m line_num)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# Convert each value according to its column and store\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m items \u001b[38;5;241m=\u001b[39m [conv(val) \u001b[38;5;28;01mfor\u001b[39;00m (conv, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(converters, vals)]\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# Then pack it according to the dtype's nesting\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m items \u001b[38;5;241m=\u001b[39m pack_items(items, packing)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gst/lib/python3.8/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong number of columns at line \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                      \u001b[38;5;241m%\u001b[39m line_num)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# Convert each value according to its column and store\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m items \u001b[38;5;241m=\u001b[39m [\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (conv, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(converters, vals)]\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# Then pack it according to the dtype's nesting\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m items \u001b[38;5;241m=\u001b[39m pack_items(items, packing)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gst/lib/python3.8/site-packages/numpy/lib/npyio.py:736\u001b[0m, in \u001b[0;36m_getconv.<locals>.floatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0x\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m.\u001b[39mfromhex(x)\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '404:'"
     ]
    }
   ],
   "source": [
    "# The url of the motorcycle data:\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/activities/motor.dat'\n",
    "# Download the data\n",
    "download(url)\n",
    "# Load the data\n",
    "data = np.loadtxt('motor.dat')\n",
    "# Figure out how many observations you have\n",
    "num_obs = data.shape[0]\n",
    "# Select what percentage you want to put in the training data\n",
    "train_percentage = 0.7\n",
    "# Figure out how many training points you are going to use:\n",
    "num_train = int(num_obs * train_percentage)\n",
    "# Figure out how many validation points you are going to use:\n",
    "num_valid = num_obs - num_train\n",
    "print('num_train = {0:d}, num_valid = {1:d}'.format(num_train, num_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before splitting the data, randomly permute rows\n",
    "permuted_data = np.random.permutation(data)\n",
    "# Now we split in a training set\n",
    "train_data = permuted_data[:num_train] # This picks the first n_train rows\n",
    "# and a validation set\n",
    "valid_data = permuted_data[num_train:] # This puts the rest on the validation rows\n",
    "# Get the x's and the y's for regression\n",
    "x_train = train_data[:, 0]\n",
    "y_train = train_data[:, 1]\n",
    "# Get the x's and the y's for validation\n",
    "x_valid = valid_data[:, 0]\n",
    "y_valid = valid_data[:, 1]\n",
    "# Let's plot the training and the validation datasets in different colors\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x_train, y_train, 'x', label='Training data')\n",
    "ax.plot(x_valid, y_valid, 'o', label='Validation data')\n",
    "ax.set_xlabel('Time (ms)')\n",
    "ax.set_ylabel('Acceleration (g)')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat here the code that calculates the design matrix for polynomial, Fourier, and radial basis functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_design_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Returns the polynomial design matrix of ``degree`` evaluated at ``x``.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    # Start with an empty list where we are going to put the columns of the matrix\n",
    "    cols = []\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(degree+1):\n",
    "        cols.append(x ** i)\n",
    "    return np.hstack(cols)\n",
    "\n",
    "def get_fourier_design_matrix(x, L, num_terms):\n",
    "    \"\"\"\n",
    "    Fourier expansion with ``num_terms`` cosines and sines.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        L           -       The \"length\" of the domain.\n",
    "        num_terms   -       How many Fourier terms do you want. This is not the number\n",
    "                            of basis functions you get. The number of basis functions\n",
    "                            is 1 + num_terms / 2. The first one is a constant.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    N = x.shape[0]\n",
    "    cols = [np.ones((N, 1))]\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(int(num_terms / 2)):\n",
    "        cols.append(np.cos(2 * (i+1) * np.pi / L * x))\n",
    "        cols.append(np.sin(2 * (i+1) * np.pi / L * x))\n",
    "    return np.hstack(cols)\n",
    "                    \n",
    "def get_rbf_design_matrix(x, x_centers, ell):\n",
    "    \"\"\"\n",
    "    Radial basis functions design matrix.\n",
    "    \n",
    "    Arguments:\n",
    "        x          -     the input points on which you want to evaluate the\n",
    "                         design matrix\n",
    "        x_center   -     the centers of the radial basis functions\n",
    "        ell        -     the lengthscale of the radial basis function\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    N = x.shape[0]\n",
    "    cols = [np.ones((N, 1))]\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(x_centers.shape[0]):\n",
    "        cols.append(np.exp(-(x - x_centers[i]) ** 2 / ell))\n",
    "    return np.hstack(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start fitting a polynomial model, plotting the response, and the prediction vs observations plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the polynomial model\n",
    "degree = 3\n",
    "Phi_poly_train = get_polynomial_design_matrix(x_train[:, None], degree)\n",
    "w_poly, _, _, _ = np.linalg.lstsq(Phi_poly_train, y_train, rcond=None)\n",
    "\n",
    "# Plot the predictions\n",
    "xx = np.linspace(0, 60, 100)\n",
    "Phi_poly_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "yy_predict = np.dot(Phi_poly_xx, w_poly)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_title('Polynomial degree {0:d}'.format(degree))\n",
    "ax.plot(x_train, y_train, 'x', label='Training data')\n",
    "ax.plot(x_valid, y_valid, 'o', label='Validation data')\n",
    "ax.plot(xx, yy_predict, '--', label='Prediction')\n",
    "ax.set_xlabel('Time (ms)')\n",
    "ax.set_ylabel('Acceleration (g)')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Make predictions on the validation data\n",
    "Phi_poly_valid = get_polynomial_design_matrix(x_valid[:, None], degree)\n",
    "y_valid_predict = np.dot(Phi_poly_valid, w_poly)\n",
    "# Calculate the mean square error\n",
    "MSE_poly = np.mean((y_valid_predict - y_valid) ** 2)\n",
    "print('MSE_poly = {0:1.2f}'.format(MSE_poly))\n",
    "# Do the predictsions vs observations plots\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_title('Polynomial degree {0:d}'.format(degree))\n",
    "ax.plot(y_valid_predict, y_valid, 'o')\n",
    "yys = np.linspace(y_valid.min(), y_valid.max(), 100)\n",
    "ax.plot(yys, yys, 'r-');\n",
    "ax.set_xlabel('Predictions')\n",
    "ax.set_ylabel('Observations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Experiment with polynomials of degree 4, 5, 10, 20\n",
    "+ When are we underfitting?\n",
    "+ When are we overfitting?\n",
    "+ Which degree (if any) gives you the best fit?\n",
    "+ Use the code blocks below to repeat this analysis for the Fourier basis and the radial basis functions. Which choice of the three basis functions seems to be better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Fourier model\n",
    "fourier_terms = 10\n",
    "fourier_L = 60.0\n",
    "Phi_fourier_train = get_fourier_design_matrix(x_train[:, None], fourier_L, fourier_terms)\n",
    "w_fourier, _, _, _ = np.linalg.lstsq(Phi_fourier_train, y_train, rcond=None)\n",
    "\n",
    "# Plot the predictions\n",
    "xx = np.linspace(0, 60, 100)\n",
    "Phi_fourier_xx = get_fourier_design_matrix(xx[:, None], fourier_L, fourier_terms)\n",
    "yy_predict = np.dot(Phi_fourier_xx, w_fourier)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_title('Fourier L={0:1.2f}, terms = {1:d}'.format(fourier_L, fourier_terms))\n",
    "ax.plot(x_train, y_train, 'x', label='Training data')\n",
    "ax.plot(x_valid, y_valid, 'o', label='Validation data')\n",
    "ax.plot(xx, yy_predict, '--', label='Prediction')\n",
    "ax.set_xlabel('Time (ms)')\n",
    "ax.set_ylabel('Acceleration (g)')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Make predictions on the validation data\n",
    "Phi_fourier_valid = get_fourier_design_matrix(x_valid[:, None], fourier_L, fourier_terms)\n",
    "y_valid_predict = np.dot(Phi_fourier_valid, w_fourier)\n",
    "# Calculate the mean square error\n",
    "MSE_fourier = np.mean((y_valid_predict - y_valid) ** 2)\n",
    "print('MSE_fourier = {0:1.2f}'.format(MSE_fourier))\n",
    "## Do the predictsions vs observations plots\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_title('Fourier L={0:1.2f}, terms = {1:d}'.format(fourier_L, fourier_terms))\n",
    "ax.plot(y_valid_predict, y_valid, 'o')\n",
    "yys = np.linspace(y_valid.min(), y_valid.max(), 100)\n",
    "ax.plot(yys, yys, 'r-');\n",
    "ax.set_xlabel('Predictions')\n",
    "ax.set_ylabel('Observations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the RBF model\n",
    "rbf_centers = np.linspace(0.0, 60.0, 10)\n",
    "rbf_ell = 10\n",
    "Phi_rbf_train = get_rbf_design_matrix(x_train[:, None], rbf_centers, rbf_ell)\n",
    "w_rbf, _, _, _ = np.linalg.lstsq(Phi_rbf_train, y_train, rcond=None)\n",
    "# Plot the function when possible\n",
    "xx = np.linspace(0, 60, 100)\n",
    "Phi_rbf_xx = get_rbf_design_matrix(xx[:, None], rbf_centers, rbf_ell)\n",
    "yy_predict = np.dot(Phi_rbf_xx, w_rbf)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_title('RBD #centers = {0:d}, ell = {1:1.2f}'.format(rbf_centers.shape[0], rbf_ell))\n",
    "ax.plot(x_train, y_train, 'x', label='Training data')\n",
    "ax.plot(x_valid, y_valid, 'o', label='Validation data')\n",
    "ax.plot(xx, yy_predict, '--', label='Prediction')\n",
    "ax.set_xlabel('Time (ms)')\n",
    "ax.set_ylabel('Acceleration (g)')\n",
    "plt.legend(loc='best')\n",
    "# Make predictions on the validation data\n",
    "Phi_rbf_valid = get_rbf_design_matrix(x_valid[:, None], rbf_centers, rbf_ell)\n",
    "y_valid_predict = np.dot(Phi_rbf_valid, w_rbf)\n",
    "# Calculate the mean square error\n",
    "MSE_rbf = np.mean((y_valid_predict - y_valid) ** 2)\n",
    "print('MSE_rbf = {0:1.2f}'.format(MSE_rbf))\n",
    "## Do the predictsions vs observations plots\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_title('RBD #centers = {0:d}, ell = {1:1.2f}'.format(rbf_centers.shape[0], rbf_ell))\n",
    "ax.plot(y_valid_predict, y_valid, 'o')\n",
    "yys = np.linspace(y_valid.min(), y_valid.max(), 100)\n",
    "ax.plot(yys, yys, 'r-');\n",
    "ax.set_xlabel('Predictions')\n",
    "ax.set_ylabel('Observations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the mean square error to select the number of basis functions\n",
    "\n",
    "Let's now use the MSE of the validation dataset to select the number of basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start increasing the number of Fourier terms and check the mean square error\n",
    "# Fit the Fourier model\n",
    "MSE = []\n",
    "for fourier_terms in range(1, 20):\n",
    "    fourier_L = 60.0\n",
    "    Phi_fourier_train = get_fourier_design_matrix(x_train[:, None], fourier_L, fourier_terms)\n",
    "    w_fourier, _, _, _ = np.linalg.lstsq(Phi_fourier_train, y_train, rcond=None)\n",
    "    # Make predictions on the validation data\n",
    "    Phi_fourier_valid = get_fourier_design_matrix(x_valid[:, None], fourier_L, fourier_terms)\n",
    "    y_valid_predict = np.dot(Phi_fourier_valid, w_fourier)\n",
    "    # Calculate the mean square error\n",
    "    MSE_fourier = np.mean((y_valid_predict - y_valid) ** 2)\n",
    "    MSE.append(MSE_fourier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have all MSE for different number of terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.bar(range(1, len(MSE)+1), MSE)\n",
    "ax.set_xlabel('Fourier terms')\n",
    "ax.set_ylabel('Mean square error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use cross validation to estimate the validation MSE.\n",
    "Remember that in cross validation we split the original dataset several times, not just once. Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "# Start increasing the number of Fourier terms and check the mean square error\n",
    "# Fit the Fourier model\n",
    "MSE = []\n",
    "num_folds = 100\n",
    "fourier_L = 60.0\n",
    "train_percentage = 0.8\n",
    "# Figure out how many training points you are going to use:\n",
    "num_train = int(num_obs * train_percentage)\n",
    "# Figure out how many validation points you are going to use:\n",
    "num_valid = num_obs - num_train\n",
    "for fourier_terms in range(1, 20):\n",
    "    mse_sum = 0.0\n",
    "    for fold in range(num_folds):\n",
    "        permuted_data = np.random.permutation(data)\n",
    "        train_data = permuted_data[:num_train]\n",
    "        valid_data = permuted_data[num_train:]\n",
    "        x_train = train_data[:, 0]\n",
    "        y_train = train_data[:, 1]\n",
    "        x_valid = valid_data[:, 0]\n",
    "        y_valid = valid_data[:, 1]\n",
    "        Phi_fourier_train = get_fourier_design_matrix(x_train[:, None], fourier_L, fourier_terms)\n",
    "        w_fourier, _, _, _ = np.linalg.lstsq(Phi_fourier_train, y_train, rcond=None)\n",
    "        # Make predictions on the validation data\n",
    "        Phi_fourier_valid = get_fourier_design_matrix(x_valid[:, None], fourier_L, fourier_terms)\n",
    "        y_valid_predict = np.dot(Phi_fourier_valid, w_fourier)\n",
    "        # Calculate the mean square error\n",
    "        MSE_fourier = np.mean((y_valid_predict - y_valid) ** 2)\n",
    "        mse_sum += MSE_fourier\n",
    "    MSE.append(mse_sum / num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.bar(range(1, len(MSE)+1), MSE)\n",
    "ax.set_xlabel('Fourier terms')\n",
    "ax.set_ylabel('Mean square error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Modify the code blocks above to use radial basis functions instead of the Fourier basis. How many terms do you need for a given lengthscale?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
